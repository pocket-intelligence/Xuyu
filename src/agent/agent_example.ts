// deep-research/agent/researchGraph.ts

import { START, END, StateGraph, Annotation } from "@langchain/langgraph";
import OpenAI from "openai";
import { readConfig } from "../configManager";
import { createOpenAIClient } from "./llm";
import readline from "readline";

// ----------------------------
// 1Ô∏è‚É£ Áä∂ÊÄÅÂÆö‰πâ
// ----------------------------
export const ResearchState = Annotation.Root({
    topic: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "",
    }),
    details: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    query: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    results: Annotation<string[]>({
        reducer: (_, x) => x,
        default: () => [],
    }),
    report: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    output_format: Annotation<"markdown" | "plain" | "json">({
        reducer: (_, x) => x,
        default: () => "markdown",
    }),
    input_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    output_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    llm_client: Annotation<OpenAI>({
        reducer: (_, x) => x,
        default: () => {
            const config = readConfig();
            if (!config || !config.llmApiUrl) throw new Error("No config found");
            return createOpenAIClient(config.llmApiUrl, config.llmApiKey);
        },
    }),
    llm_model: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "deepseek-v3.1",
    }),
});

// ----------------------------
// Â∑•ÂÖ∑ÂáΩÊï∞ÔºöÂëΩ‰ª§Ë°å‰∫§‰∫í
// ----------------------------
function askUser(question: string): Promise<string> {
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout,
    });
    return new Promise((resolve) =>
        rl.question(question, (answer) => {
            rl.close();
            resolve(answer.trim());
        }),
    );
}

// ----------------------------
// 2Ô∏è‚É£ ËäÇÁÇπÂáΩÊï∞
// ----------------------------
export async function askDetails(state: typeof ResearchState.State) {
    const prompt = `‰Ω†ÊòØ‰∏Ä‰∏™Á†îÁ©∂Âä©Êâã„ÄÇËØ∑Â∞±‚Äú${state.topic}‚ÄùËøô‰∏™‰∏ªÈ¢òÔºåÂêëÁî®Êà∑ÊèêÂá∫3-5‰∏™ÂÖ≥ÈîÆÈóÆÈ¢òÔºå‰ª•Â∏ÆÂä©‰Ω†Êõ¥Â•ΩÂú∞ËøõË°åÁ†îÁ©∂„ÄÇ`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [
            { role: "system", content: "‰Ω†ÊòØ‰∏Ä‰∏™Á†îÁ©∂Âä©Êâã„ÄÇ" },
            { role: "user", content: prompt },
        ],
    });

    return {
        details: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

/** Áî®Êà∑Âπ≤È¢ÑËäÇÁÇπÔºöÂÖÅËÆ∏‰øÆÊîπ‰∏ªÈ¢òÊàñÁªÜËäÇ */
export async function userReviewDetails(state: typeof ResearchState.State) {
    console.log("\nüß© Á≥ªÁªüÁîüÊàêÁöÑÁ†îÁ©∂ÁªÜËäÇÔºö\n");
    console.log(state.details);

    const edit = await askUser("\nÊòØÂê¶‰øÆÊîπÁ†îÁ©∂ÁªÜËäÇÊàñ‰∏ªÈ¢òÔºü(y/n): ");
    let topic = state.topic;
    let details = state.details;

    if (edit.toLowerCase() === "y") {
        const newTopic = await askUser(`ËØ∑ËæìÂÖ•Êñ∞ÁöÑ‰∏ªÈ¢òÔºàÊàñÂõûËΩ¶‰øùÊåÅ‚Äú${state.topic}‚ÄùÔºâ: `);
        if (newTopic) topic = newTopic;

        const newDetails = await askUser(`ËØ∑ËæìÂÖ•Êñ∞ÁöÑÁªÜËäÇÔºàÊàñÂõûËΩ¶‰øùÊåÅÂΩìÂâçÂÜÖÂÆπÔºâ: `);
        if (newDetails) details = newDetails;
    }

    return { topic, details };
}

/** Ê†πÊçÆÁ†îÁ©∂‰∏ªÈ¢òÊûÑÂª∫Ê£ÄÁ¥¢ÂÖ≥ÈîÆËØç */
export async function buildQuery(state: typeof ResearchState.State) {
    const prompt = `ËØ∑Ê†πÊçÆÁ†îÁ©∂‰∏ªÈ¢ò‚Äú${state.topic}‚ÄùÂíåË°•ÂÖÖÁªÜËäÇ‚Äú${state.details}‚ÄùÔºåÁîüÊàê3-5‰∏™ÈÄÇÂêàÂ≠¶ÊúØÊêúÁ¥¢ÁöÑËã±ÊñáÂÖ≥ÈîÆËØç„ÄÇ`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        query: resp.choices[0].message.content?.trim().replace(/\n/g, " ") || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

/** Áî®Êà∑Âπ≤È¢ÑËäÇÁÇπÔºöÈÄâÊã©ËæìÂá∫Ê†ºÂºè */
export async function userChooseFormat(state: typeof ResearchState.State) {
    console.log("\nüîç Á≥ªÁªüÁîüÊàêÁöÑÊêúÁ¥¢ÂÖ≥ÈîÆËØçÔºö", state.query);
    const format = await askUser("\nËØ∑ÈÄâÊã©ËæìÂá∫Ê†ºÂºèÔºàmarkdown/plain/jsonÔºåÈªòËÆ§ markdownÔºâ: ");
    const selected =
        format.toLowerCase() === "plain"
            ? "plain"
            : format.toLowerCase() === "json"
                ? "json"
                : "markdown";

    return { output_format: selected };
}

/** ÊêúÁ¥¢ SearxNG ÁªìÊûú */
export async function searchSearxng(state: typeof ResearchState.State) {
    if (!state.query) return {};
    const searxUrl = "http://localhost:9527/search";
    const params = new URLSearchParams({ q: state.query, format: "json" });
    const resp = await fetch(`${searxUrl}?${params.toString()}`);
    const data = await resp.json();

    const results = (data.results || []).slice(0, 5).map(
        (r: any, i: number) =>
            `#${i + 1}: ${r.title}\n${r.url}\n${r.content}`,
    );
    return { results };
}

/** ÁîüÊàêÊä•Âëä */
export async function writeReport(state: typeof ResearchState.State) {
    const formatHint =
        state.output_format === "markdown"
            ? "ËØ∑Áî® Markdown Ê†ºÂºèËæìÂá∫"
            : state.output_format === "json"
                ? "ËØ∑ËæìÂá∫ JSON Ê†ºÂºèÔºåÂåÖÊã¨ title, summary, trends ‰∏â‰∏™Â≠óÊÆµ"
                : "ËØ∑Áî®Á∫ØÊñáÊú¨Ê†ºÂºèËæìÂá∫";

    const prompt = `
ËØ∑Ê†πÊçÆ‰ª•‰∏ã‰ø°ÊÅØÊí∞ÂÜô‰∏Ä‰ªΩÁÆÄÁü≠Á†îÁ©∂Êä•Âëä„ÄÇ

‰∏ªÈ¢òÔºö${state.topic}
ÁªÜËäÇÔºö${state.details}
ÊêúÁ¥¢ÁªìÊûúÔºö
${state.results.join("\n\n")}

Ë¶ÅÊ±ÇÔºö
- Áî®ÁÆÄÊ¥Å„ÄÅÈÄªËæëÊ∏ÖÊô∞ÁöÑ‰∏≠ÊñáÊí∞ÂÜô
- Ê¶ÇËø∞Á†îÁ©∂Áé∞Áä∂„ÄÅË∂ãÂäøÂíåÊΩúÂú®ÊñπÂêë
- ${formatHint}
`;

    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        report: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

// ----------------------------
// 3Ô∏è‚É£ ÊûÑÂª∫ÈìæÂºè Graph
// ----------------------------
export const ResearchGraph = new StateGraph(ResearchState)
    .addNode("askDetails", askDetails)
    .addNode("userReviewDetails", userReviewDetails)
    .addNode("buildQuery", buildQuery)
    .addNode("userChooseFormat", userChooseFormat)
    .addNode("search", searchSearxng)
    .addNode("writeReport", writeReport)
    .addEdge(START, "askDetails")
    .addEdge("askDetails", "userReviewDetails")
    .addEdge("userReviewDetails", "buildQuery")
    .addEdge("buildQuery", "userChooseFormat")
    .addEdge("userChooseFormat", "search")
    .addEdge("search", "writeReport")
    .addEdge("writeReport", END);

export const app = ResearchGraph.compile();

// ----------------------------
// 4Ô∏è‚É£ ÊµãËØïÊâßË°å
// ----------------------------
if (require.main === module) {
    (async () => {
        const config = readConfig();
        const initState = {
            topic: "AI ethics in generative models",
            details: "",
            query: "",
            results: [] as string[],
            report: "",
            output_format: "markdown" as const,
            input_tokens: 0,
            output_tokens: 0,
            llm_client: createOpenAIClient(config.llmApiUrl, config.llmApiKey),
        };

        const result = await app.invoke(initState);
        console.log("\n=== üß† ÊúÄÁªàÁ†îÁ©∂Êä•Âëä ===\n");
        console.log(result.report);
    })();
}
