// deep-research/agent/researchGraph.ts

import { START, END, StateGraph, Annotation } from "@langchain/langgraph";
import OpenAI from "openai";
import { readConfig } from "../configManager";
import { createOpenAIClient } from "./llm";

export const ResearchState = Annotation.Root({
    topic: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "",
    }),
    details: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    query: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    results: Annotation<string[]>({
        reducer: (_, x) => x,
        default: () => [],
    }),
    report: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    input_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    output_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    llm_client: Annotation<OpenAI>({
        reducer: (_, x) => x,
        default: () => {
            const config = readConfig();
            if (!config || !config.llmApiUrl) throw new Error("No config found");
            return createOpenAIClient(config.llmApiUrl, config.llmApiKey);
        },
    }),
    llm_model: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "deepseek-v3.1",
    }),
});

// ----------------------------
// 2ï¸âƒ£ èŠ‚ç‚¹å‡½æ•°
// ----------------------------
// ----------------------------
// 2ï¸âƒ£ èŠ‚ç‚¹å‡½æ•°
// ----------------------------
export async function askDetails(state: typeof ResearchState.State): Promise<Partial<typeof ResearchState.State>> {
    const prompt = `ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚è¯·å°±â€œ${state.topic}â€è¿™ä¸ªä¸»é¢˜ï¼Œå‘ç”¨æˆ·æå‡º3-5ä¸ªå…³é”®é—®é¢˜ï¼Œä»¥å¸®åŠ©ä½ æ›´å¥½åœ°è¿›è¡Œç ”ç©¶ã€‚`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [
            { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚" },
            { role: "user", content: prompt },
        ],
    });

    return {
        details: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

export async function buildQuery(state: typeof ResearchState.State): Promise<Partial<typeof ResearchState.State>> {
    const prompt = `è¯·æ ¹æ®ç ”ç©¶ä¸»é¢˜â€œ${state.topic}â€å’Œè¡¥å……ç»†èŠ‚â€œ${state.details}â€ï¼Œç”Ÿæˆ3-5ä¸ªé€‚åˆå­¦æœ¯æœç´¢çš„è‹±æ–‡å…³é”®è¯ã€‚`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        query: resp.choices[0].message.content?.trim().replace(/\n/g, " ") || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

export async function searchSearxng(state: typeof ResearchState.State): Promise<Partial<typeof ResearchState.State>> {
    if (!state.query) return {};

    // ğŸ”§ æ›¿æ¢ä¸ºä½ çš„ SearxNG API
    const searxUrl = "http://localhost:9527/search";
    const params = new URLSearchParams({ q: state.query, format: "json" });
    const resp = await fetch(`${searxUrl}?${params.toString()}`);
    const data = await resp.json();

    const results = (data.results || []).slice(0, 5).map((r: any) => `${r.title}\n${r.url}\n${r.content}`);
    return { results };
}

export async function writeReport(state: typeof ResearchState.State): Promise<Partial<typeof ResearchState.State>> {
    const prompt = `
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ’°å†™ä¸€ä»½ç®€çŸ­ç ”ç©¶æŠ¥å‘Šã€‚

ä¸»é¢˜ï¼š${state.topic}
ç»†èŠ‚ï¼š${state.details}
æœç´¢ç»“æœï¼š
${state.results.join("\n\n")}

è¦æ±‚ï¼š
- ç”¨ç®€æ´ã€é€»è¾‘æ¸…æ™°çš„ä¸­æ–‡æ’°å†™
- æ¦‚è¿°ç ”ç©¶ç°çŠ¶ã€è¶‹åŠ¿å’Œæ½œåœ¨æ–¹å‘
`;

    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        report: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

// ----------------------------
// 3ï¸âƒ£ æ„å»ºé“¾å¼ Graph
// ----------------------------
export const ResearchGraph = new StateGraph(ResearchState)
    .addNode("askDetails", askDetails)
    .addNode("buildQuery", buildQuery)
    .addNode("search", searchSearxng)
    .addNode("writeReport", writeReport)
    .addEdge(START, "askDetails")
    .addEdge("askDetails", "buildQuery")
    .addEdge("buildQuery", "search")
    .addEdge("search", "writeReport");

// å¦‚æœéœ€è¦æ¡ä»¶è·³è½¬ï¼Œå¯ä»¥è¿™æ ·
// .addConditionalEdges("writeReport", (state) => {
//   return state.detailsChanged ? "askDetails" : END;
// });

export const app = ResearchGraph.compile();


// ----------------------------
// 4ï¸âƒ£ æµ‹è¯•æ‰§è¡Œ
// ----------------------------
if (require.main === module) {
    (async () => {
        const config = readConfig();
        const initState = {
            topic: "AI ethics in generative models",
            details: "",
            query: "",
            results: [] as string[],
            report: "",
            input_tokens: 0,
            output_tokens: 0,
            llm_client: createOpenAIClient(config.llmApiUrl, config.llmApiKey),
        };
        const result = await app.invoke(initState);

        console.log("\n=== ç ”ç©¶æŠ¥å‘Š ===\n");
        console.log(result.report);
    })();
}