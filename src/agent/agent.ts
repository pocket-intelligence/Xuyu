// deep-research/agent/researchGraph.ts

import { START, END, StateGraph, Annotation } from "@langchain/langgraph";
import OpenAI from "openai";
import { readConfig } from "../configManager";
import { createOpenAIClient } from "./llm";
import readline from "readline";
import { ipcRenderer } from "electron";

// ----------------------------
// 1ï¸âƒ£ çŠ¶æ€å®šä¹‰
// ----------------------------
export const ResearchState = Annotation.Root({
    topic: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "",
    }),
    details: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    query: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    results: Annotation<string[]>({
        reducer: (_, x) => x,
        default: () => [],
    }),
    report: Annotation<string | null>({
        reducer: (_, x) => x,
        default: () => null,
    }),
    output_format: Annotation<"markdown" | "plain" | "json">({
        reducer: (_, x) => x,
        default: () => "markdown",
    }),
    input_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    output_tokens: Annotation<number>({
        reducer: (x, y) => x + (y ?? 0),
        default: () => 0,
    }),
    llm_client: Annotation<OpenAI>({
        reducer: (_, x) => x,
        default: () => {
            const config = readConfig();
            if (!config || !config.llmApiUrl) throw new Error("No config found");
            return createOpenAIClient(config.llmApiUrl, config.llmApiKey);
        },
    }),
    llm_model: Annotation<string>({
        reducer: (_, x) => x,
        default: () => "deepseek-v3.1",
    }),
});

// ----------------------------
// å·¥å…·å‡½æ•°ï¼šå‘½ä»¤è¡Œäº¤äº’, ç”¨äºæµ‹è¯•
// ----------------------------
function askUserInConsole(question: string): Promise<string> {
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout,
    });
    return new Promise((resolve) =>
        rl.question(question, (answer) => {
            rl.close();
            resolve(answer.trim());
        }),
    );
}


// æ¸²æŸ“è¿›ç¨‹ç”¨è¿™ä¸ªç‰ˆæœ¬
export async function askUser(question: string): Promise<string> {
    return await ipcRenderer.invoke("ask-user", question);
}


// ----------------------------
// 2ï¸âƒ£ èŠ‚ç‚¹å‡½æ•°
// ----------------------------
export async function askDetails(state: typeof ResearchState.State) {
    const prompt = `ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚è¯·å°±â€œ${state.topic}â€è¿™ä¸ªä¸»é¢˜ï¼Œå‘ç”¨æˆ·æå‡º3-5ä¸ªå…³é”®é—®é¢˜ï¼Œä»¥å¸®åŠ©ä½ æ›´å¥½åœ°è¿›è¡Œç ”ç©¶ã€‚`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [
            { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚" },
            { role: "user", content: prompt },
        ],
    });

    return {
        details: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

/** ç”¨æˆ·å¹²é¢„èŠ‚ç‚¹ï¼šå…è®¸ä¿®æ”¹ä¸»é¢˜æˆ–ç»†èŠ‚ */
export async function userReviewDetails(state: typeof ResearchState.State) {
    console.log("\nğŸ§© ç³»ç»Ÿç”Ÿæˆçš„ç ”ç©¶ç»†èŠ‚ï¼š\n");
    console.log(state.details);

    const edit = await askUser("\næ˜¯å¦ä¿®æ”¹ç ”ç©¶ç»†èŠ‚æˆ–ä¸»é¢˜ï¼Ÿ(y/n): ");
    let topic = state.topic;
    let details = state.details;

    if (edit.toLowerCase() === "y") {
        const newTopic = await askUser(`è¯·è¾“å…¥æ–°çš„ä¸»é¢˜ï¼ˆæˆ–å›è½¦ä¿æŒâ€œ${state.topic}â€ï¼‰: `);
        if (newTopic) topic = newTopic;

        const newDetails = await askUser(`è¯·è¾“å…¥æ–°çš„ç»†èŠ‚ï¼ˆæˆ–å›è½¦ä¿æŒå½“å‰å†…å®¹ï¼‰: `);
        if (newDetails) details = newDetails;
    }

    return { topic, details };
}

/** æ ¹æ®ç ”ç©¶ä¸»é¢˜æ„å»ºæ£€ç´¢å…³é”®è¯ */
export async function buildQuery(state: typeof ResearchState.State) {
    const prompt = `è¯·æ ¹æ®ç ”ç©¶ä¸»é¢˜â€œ${state.topic}â€å’Œè¡¥å……ç»†èŠ‚â€œ${state.details}â€ï¼Œç”Ÿæˆ3-5ä¸ªé€‚åˆå­¦æœ¯æœç´¢çš„è‹±æ–‡å…³é”®è¯ã€‚`;
    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        query: resp.choices[0].message.content?.trim().replace(/\n/g, " ") || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

/** ç”¨æˆ·å¹²é¢„èŠ‚ç‚¹ï¼šé€‰æ‹©è¾“å‡ºæ ¼å¼ */
export async function userChooseFormat(state: typeof ResearchState.State) {
    console.log("\nğŸ” ç³»ç»Ÿç”Ÿæˆçš„æœç´¢å…³é”®è¯ï¼š", state.query);
    const format = await askUser("\nè¯·é€‰æ‹©è¾“å‡ºæ ¼å¼ï¼ˆmarkdown/plain/jsonï¼Œé»˜è®¤ markdownï¼‰: ");
    const selected =
        format.toLowerCase() === "plain"
            ? "plain"
            : format.toLowerCase() === "json"
                ? "json"
                : "markdown";

    return { output_format: selected };
}

/** æœç´¢ SearxNG ç»“æœ */
export async function searchSearxng(state: typeof ResearchState.State) {
    if (!state.query) return {};
    const searxUrl = "http://localhost:9527/search";
    const params = new URLSearchParams({ q: state.query, format: "json" });
    const resp = await fetch(`${searxUrl}?${params.toString()}`);
    const data = await resp.json();

    const results = (data.results || []).slice(0, 5).map(
        (r: any, i: number) =>
            `#${i + 1}: ${r.title}\n${r.url}\n${r.content}`,
    );
    return { results };
}

/** ç”ŸæˆæŠ¥å‘Š */
export async function writeReport(state: typeof ResearchState.State) {
    const formatHint =
        state.output_format === "markdown"
            ? "è¯·ç”¨ Markdown æ ¼å¼è¾“å‡º"
            : state.output_format === "json"
                ? "è¯·è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…æ‹¬ title, summary, trends ä¸‰ä¸ªå­—æ®µ"
                : "è¯·ç”¨çº¯æ–‡æœ¬æ ¼å¼è¾“å‡º";

    const prompt = `
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ’°å†™ä¸€ä»½ç®€çŸ­ç ”ç©¶æŠ¥å‘Šã€‚

ä¸»é¢˜ï¼š${state.topic}
ç»†èŠ‚ï¼š${state.details}
æœç´¢ç»“æœï¼š
${state.results.join("\n\n")}

è¦æ±‚ï¼š
- ç”¨ç®€æ´ã€é€»è¾‘æ¸…æ™°çš„ä¸­æ–‡æ’°å†™
- æ¦‚è¿°ç ”ç©¶ç°çŠ¶ã€è¶‹åŠ¿å’Œæ½œåœ¨æ–¹å‘
- ${formatHint}
`;

    const resp = await state.llm_client.chat.completions.create({
        model: state.llm_model,
        messages: [{ role: "user", content: prompt }],
    });

    return {
        report: resp.choices[0].message.content?.trim() || "",
        input_tokens: resp.usage?.prompt_tokens ?? 0,
        output_tokens: resp.usage?.completion_tokens ?? 0,
    };
}

// ----------------------------
// 3ï¸âƒ£ æ„å»ºé“¾å¼ Graph
// ----------------------------
export const ResearchGraph = new StateGraph(ResearchState)
    .addNode("askDetails", askDetails)
    .addNode("userReviewDetails", userReviewDetails)
    .addNode("buildQuery", buildQuery)
    .addNode("userChooseFormat", userChooseFormat)
    .addNode("search", searchSearxng)
    .addNode("writeReport", writeReport)
    .addEdge(START, "askDetails")
    .addEdge("askDetails", "userReviewDetails")
    .addEdge("userReviewDetails", "buildQuery")
    .addEdge("buildQuery", "userChooseFormat")
    .addEdge("userChooseFormat", "search")
    .addEdge("search", "writeReport")
    .addEdge("writeReport", END);

export const app = ResearchGraph.compile();

// ----------------------------
// 4ï¸âƒ£ æµ‹è¯•æ‰§è¡Œ
// ----------------------------
if (require.main === module) {
    (async () => {
        const config = readConfig();
        const initState = {
            topic: "AI ethics in generative models",
            details: "",
            query: "",
            results: [] as string[],
            report: "",
            output_format: "markdown" as const,
            input_tokens: 0,
            output_tokens: 0,
            llm_client: createOpenAIClient(config.llmApiUrl, config.llmApiKey),
        };

        const result = await app.invoke(initState);
        console.log("\n=== ğŸ§  æœ€ç»ˆç ”ç©¶æŠ¥å‘Š ===\n");
        console.log(result.report);
    })();
}
